{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8n8o6UyM6KR"
   },
   "source": [
    "# Softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://nbviewer.jupyter.org/github/hongjiaherng/ML-Collections/blob/main/just4funml/notes/note_softmax_regression.ipynb\" \n",
    "   target=\"_parent\">\n",
    "   <img align=\"left\" \n",
    "      src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" \n",
    "      width=\"109\" height=\"20\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnTTrp7VNNZn"
   },
   "source": [
    "### 1. Brief explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8E7KziAM71E"
   },
   "source": [
    "`Softmax regression`, or `Multinomial logistic regression` is the generalized version of logistic regression which can perform multiclass classification directly, unlike regular logistic regression which needs to use one-versus-all technique to enable multiclass classification.\n",
    "<br><br>\n",
    "\n",
    "Given an instance $x$, this model computes a score $s_k(x)$ for each class $k$ on the instance. Then, `softmax function` is applied to the scores to obtain the probability of instance $x$ belongs to every class. Finally, the model chooses the class which has the highest probability and classify instance $x$ to that particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMusLRxyNDX0"
   },
   "source": [
    "### 2. Model hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x90tiCRON1Ab"
   },
   "source": [
    "The following formulas in this part are all based on 1 training example\n",
    "\n",
    "Let first define:<br>\n",
    "$\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix}$ &nbsp;&nbsp;&nbsp;\n",
    "$\\theta^{(k)} =\n",
    "\\begin{bmatrix}\n",
    "\\theta^{(k)}_0 \\\\\n",
    "\\theta^{(k)}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta^{(k)}_n \\\\\n",
    "\\end{bmatrix}\n",
    "$<br>\n",
    "$\n",
    "\\Theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta^{(1)}_0 & \\theta^{(2)}_0 & ... & \\theta^{(K)}_0 \\\\\n",
    "\\theta^{(1)}_1 & \\theta^{(2)}_1 & ... & \\theta^{(K)}_1 \\\\\n",
    "\\theta^{(1)}_2 & \\theta^{(2)}_2 & ... & \\theta^{(K)}_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\theta^{(1)}_n & \\theta^{(2)}_n & ... & \\theta^{(K)}_n \\\\\n",
    "\\end{bmatrix} =\n",
    "\\left[\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right]\n",
    "$\n",
    "<br><br>\n",
    "where<br>\n",
    "$n$ = number of features, <br>\n",
    "$K$ = number of classes, <br>\n",
    "$\\mathbf{x} \\in \\mathbb{R}^{(n+1)\\times1}$ &nbsp;, (n+1-dimensional vector include bias term)<br>\n",
    "$\\Theta \\in \\mathbb{R}^{(n+1)\\times K}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjhnlWSCOBvS"
   },
   "source": [
    "#### A) ***Softmax score for class k of instance $x$ (aka logit)*** \n",
    "    \n",
    "- Compute this for all class k where k = 1, 2, ..., k\n",
    "<br><br>\n",
    "$s_k(\\mathbf{x}) = \\theta^{(k)\\top}\\mathbf{x}$\n",
    "\n",
    "<br>where <br>\n",
    "$s_k(\\mathbf{x}) \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkoyBnrqONMi"
   },
   "source": [
    "#### B) ***Softmax function*** \n",
    "- Compute this for all class k where k = 1, 2, ..., k\n",
    "<br><br>\n",
    "$\n",
    "P(y = k|\\mathbf{x};\\theta) =  \n",
    "\\hat{p}_k = \n",
    "\\sigma(s(\\mathbf{x}))_k = \n",
    "\\dfrac{\\exp(s_k(\\mathbf{x}))}{\\sum\\limits_{j=1}^{K}{\\exp(s_j(\\mathbf{x}))}} =\n",
    "\\dfrac{ \\exp(\\theta^{(k)\\top}\\mathbf{x}) }{ \\sum\\limits_{j=1}^{K} \\exp(\\theta^{(j)\\top}\\mathbf{x})}\n",
    "$\n",
    "\n",
    "where <br>\n",
    "$K$ = number of classes, <br>\n",
    "$\\sum\\limits_{j=1}^{K}{\\exp(s_j(\\mathbf{x}))}$ = the sum of exponential of softmax scores for every class of instance $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) ***Model hypothesis***\n",
    "- Probability distribution of instance $\\mathbf{x}$ belong to a class $k$\n",
    "- This sums to 1\n",
    "\n",
    "$h_{\\theta}(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1|\\mathbf{x};\\theta) \\\\\n",
    "P(y = 2|\\mathbf{x};\\theta) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K|\\mathbf{x};\\theta)\n",
    "\\end{bmatrix} =\n",
    "\\dfrac{1}{\\sum\\limits_{j=1}^{K} \\exp(\\theta^{(j)\\top}\\mathbf{x})}\n",
    "\\begin{bmatrix}\n",
    "\\exp(\\theta^{(1)\\top}\\mathbf{x}) \\\\\n",
    "\\exp(\\theta^{(2)\\top}\\mathbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp(\\theta^{(K)\\top}\\mathbf{x})\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGk-sX11X0Xb"
   },
   "source": [
    "#### D) ***Prediction***\n",
    "- This function obtains the greatest probability in the vector and return its corresponding class number $k$\n",
    "<br><br>\n",
    "$\\hat{y} = argmax_{k}\\; h_{\\theta}(\\mathbf{x})$\n",
    "\n",
    "where <br>\n",
    "$\\hat{y}$ = prediction of instance $\\mathbf{x}$ (class's number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulraatYXYAKb"
   },
   "source": [
    "### 3. Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\Theta) = - \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} 1\\{y^{(i)} = k\\} \\log P(y^{(i)} = k|\\mathbf{x}^{(i)};\\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cost function for softmax regression is called `Cross entropy loss`. \n",
    "\n",
    "- Why this cost function? (Imagine this)\n",
    "    - If $y^{(i)} == k$, and it predict high probability that $y^{(i)} == k$, that means $P(y = k|\\mathbf{x};\\theta)$ is high, then $J(\\Theta)$ will be low\n",
    "    - If $y^{(i)} == k$, and it predict low probability that $y^{(i)} == k$, that means $P(y = k|\\mathbf{x};\\theta)$ is low, then $J(\\Theta)$ will be high\n",
    "- This cost function is **convex** (but I dunno how to determine yet ;) ), so running gradient descent will find the global optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimizing cost\n",
    "So we want to find the derivative of $J(\\Theta)$ w.r.t every $\\theta_j^{(k)}$ to get its gradient, in particular $\\dfrac{\\partial J(\\Theta)}{\\partial\\theta_j^{(k)}} $\n",
    "- $\\nabla J(\\Theta)$ have a shape of $(n + 1) \\times k$\n",
    "\n",
    "$\\nabla J(\\Theta) = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial J}{\\partial\\theta_0^{(1)}} & \\dfrac{\\partial J}{\\partial\\theta_0^{(2)}} & ... & \\dfrac{\\partial J}{\\partial\\theta_0^{(K)}} \\\\\n",
    "\\dfrac{\\partial J}{\\partial\\theta_1^{(1)}} & \\dfrac{\\partial J}{\\partial\\theta_1^{(2)}} & ... & \\dfrac{\\partial J}{\\partial\\theta_1^{(K)}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial J}{\\partial\\theta_n^{(1)}} & \\dfrac{\\partial J}{\\partial\\theta_n^{(2)}} & ... & \\dfrac{\\partial J}{\\partial\\theta_n^{(K)}} \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "| & | & ... & | \\\\\n",
    "\\nabla_{\\theta^{(1)}} J(\\Theta) & \\nabla_{\\theta^{(2)}} J(\\Theta) & ... & \\nabla_{\\theta^{(K)}} J(\\Theta) \\\\\n",
    "| & | & ... & | \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "It turns out that in this case we can just find $J(\\Theta)$ w.r.t entire $\\theta^{(k)}$, in particular $\\nabla_{\\theta^{(k)}} J(\\Theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivation\n",
    "First, let's simplify the cost function first for easier derivation\n",
    "- 2nd -> 3rd row\n",
    "    - Front part: Consider $y^{(i)}$ only equals to $k$ once per instance, unmatch condition is treated as $0 \\times \\log \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}}$, so can safely ignore them\n",
    "    - Back part: sum over K has nothing to do with j in $\\log \\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}}$, so we can pull it out from summation\n",
    "- 3rd -> 4th row\n",
    "    - Back part: $y^{(i)}$  will be equal to one of the class k, so the sum of that is only 1 (consider 1 + 0 + 0 + ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\Theta) = \n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} 1\\{y^{(i)} = k\\} \\log \\dfrac{ \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}} }{ \\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}}} \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg\\{ \\bigg[\\sum\\limits_{k=1}^{K} 1\\{y^{(i)}  = k\\} \\log \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}}\\bigg] - \\bigg[\\sum\\limits_{k=1}^{K} 1\\{y^{(i)}  = k\\} \\log \\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}} \\bigg] \\bigg\\} \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg\\{ \\bigg[ 1\\{y^{(i)}  = k\\} \\log \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}}\\bigg] - \\bigg[ \\log \\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}} \\sum\\limits_{k=1}^{K} 1\\{y^{(i)}  = k\\}  \\bigg] \\bigg\\} \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg\\{ \\bigg[ 1\\{y^{(i)}  = k\\} \\log \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}}\\bigg] - \\bigg[ \\log (\\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}}) (1)  \\bigg] \\bigg\\} \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg[ 1\\{y^{(i)}  = k\\} \\log \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}} -  \\log (\\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}})  \\bigg] \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the derivative w.r.t. $\\theta^{(k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{\\theta^{(k)}} J(\\Theta) = \n",
    "\\nabla_{\\theta^{(k)}} \\bigg\\{ - \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg[ 1\\{y^{(i)}  = k\\} \\log \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}} -  \\log (\\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}})  \\bigg] \\bigg\\} \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg[\\nabla_{\\theta^{(k)}} 1\\{y^{(i)}  = k\\} \\log \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}} - \\nabla_{\\theta^{(k)}} \\log (\\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}})  \\bigg] \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg[1\\{y^{(i)}  = k\\} \\nabla_{\\theta^{(k)}}  \\log \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}} - \\dfrac{1}{\\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}}} \\nabla_{\\theta^{(k)}} \\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}} \\bigg] \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg[1\\{y^{(i)}  = k\\} \\nabla_{\\theta^{(k)}}  \\theta^{(k)\\top}\\mathbf{x}^{(i)} - \\dfrac{1}{\\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}}} \\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}} \\nabla_{\\theta^{(k)}} {\\theta^{(k)\\top}\\mathbf{x}^{(i)}}  \\bigg] \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg[1\\{y^{(i)}  = k\\} \\mathbf{x}^{(i)} - \\dfrac{\\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}}}{\\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}}}   \\mathbf{x}^{(i)}  \\bigg] \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg[\\mathbf{x}^{(i)} \\bigg( 1\\{y^{(i)}  = k\\} - \\dfrac{\\mathbf{e}^{\\theta^{(k)\\top}\\mathbf{x}^{(i)}}}{\\sum\\limits_{j=1}^{K} \\mathbf{e}^{\\theta^{(j)\\top}\\mathbf{x}^{(i)}}} \\bigg) \\bigg] \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; =\n",
    "- \\dfrac{1}{m} \\sum\\limits_{i=1}^{m} \\bigg[\\mathbf{x}^{(i)} \\bigg( 1\\{y^{(i)}  = k\\} - P(y^{(i)} = k|\\mathbf{x}^{(i)};\\theta) \\bigg) \\bigg]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's is the simpler way to write the cross entropy cost function (Easier for implementation)\n",
    "- We one hot encode y (something like below, where the shape of it is m x K)\n",
    "\n",
    "$\n",
    "Y\\_one\\_hot =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & 1 & ... & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 1 & 0 & ... & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nu4jJWheaCIX"
   },
   "source": [
    "**Cross entropy cost function (Unregularized)**\n",
    "\n",
    "$J(\\Theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lKgAA22_4cI"
   },
   "source": [
    "$\n",
    "\\nabla_{\\theta^{(k)}} J(\\theta) =\n",
    "- \\frac{1}{m} \\sum_\\limits{i=1}^{m} (y_k^{(i)} - \\hat{p}_k^{(i)}) \\mathbf{x}^{(i)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vectorized implementation\n",
    "cost = (- 1 / m) * np.sum(Y_one_hot * np.log(Y_proba_predict))\n",
    "gradients = (- 1 / m) * (X_with_bias.T @ (Y_one_hot - Y_proba_predict))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmwrssYQdxYu"
   },
   "source": [
    "**Cross entropy cost function with $l_1$ regularization**\n",
    "\n",
    "$J(\\Theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)}) + \\lambda \\sum\\limits_{j=1}^{n} \\sum\\limits_{k=1}^{K} |\\theta_j^{(k)}| $ &nbsp;&nbsp;&nbsp;&nbsp;, regularization exclude j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwZaQnJN-N4i"
   },
   "source": [
    "$\n",
    "\\nabla_{\\theta^{(k)}} J(\\theta) = \n",
    "- \\frac{1}{m} \\sum_\\limits{i=1}^{m} (y_k^{(i)} - \\hat{p}_k^{(i)}) \\mathbf{x}^{(i)} + \\lambda \\cdot sign({\\color{red}{\\theta^{(k)}}})\n",
    "\\;\\;\\;\n",
    "$\n",
    "\n",
    "where<br>\n",
    "$sign({\\color{red}{\\theta^{(k)}}}) = \n",
    "\\begin{bmatrix}\n",
    "{\\color{red}0} \\\\\n",
    "sign(\\theta^{(k)}_1) \\\\\n",
    "\\vdots \\\\\n",
    "sign(\\theta^{(k)}_n) \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "where\n",
    "$\n",
    "sign(\\theta^{(k)}_j) = \n",
    "\\left\\{\\begin{matrix}\n",
    "-1 & if \\; \\; \\theta^{(k)}_j < 0 \\\\ \n",
    "0 & if \\; \\;  \\theta^{(k)}_j = 0 \\\\ \n",
    "+1 & if \\; \\;  \\theta^{(k)}_j > 0\n",
    "\\end{matrix}\\right.\n",
    "$\n",
    "<br><br>\n",
    "The weights ($\\theta^{(k)}_0$) of all bias term $x_0 = 1$ are not being penalized in regularization, thus replace the first row of $\\Theta$ to 0 to avoid being regularized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vectorized implementation\n",
    "cost = (- 1 / m) * np.sum(Y_one_hot * np.log(Y_proba_predict))\n",
    "l1_loss = alpha * np.sum(np.abs(Theta[1:]))\n",
    "cost += l1_loss\n",
    "\n",
    "gradients = (- 1 / m) * (X_with_bias.T @ (Y_one_hot - Y_proba_predict)) + alpha * np.r_[np.zeros((1, K)), np.sign(Theta[1:])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3fyaNgvbN3B"
   },
   "source": [
    "**Cross entropy cost function with $l_2$ regularization**\n",
    "\n",
    "$J(\\Theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)}) + \\frac{\\lambda}{2} \\sum\\limits_{j=1}^{n} \\sum\\limits_{k=1}^{K} (\\theta_j^{(k)})^2 $ &nbsp;&nbsp;&nbsp;&nbsp;, regularization exclude j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ0EvHf1-M8I"
   },
   "source": [
    "$\n",
    "\\nabla_{\\theta^{(k)}} J(\\theta) = \n",
    "- \\frac{1}{m} \\sum_\\limits{i=1}^{m} (y_k^{(i)} - \\hat{p}_k^{(i)}) \\mathbf{x}^{(i)} + \\lambda \\color{red}{\\theta^{(k)}}\n",
    "$\n",
    "\n",
    "where<br>\n",
    "$\\color{red}{\\theta^{(k)}} = \n",
    "\\begin{bmatrix}\n",
    "\\color{red}0 \\\\\n",
    "\\theta^{(k)}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta^{(k)}_n \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<br>\n",
    "The weights ($\\theta^{(k)}_0$) of all bias term $x_0 = 1$ are not being penalized in regularization, thus replace the first row of $\\Theta$ to 0 to avoid being regularized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vectorized implementation\n",
    "cost = (- 1 / m) * np.sum(Y_one_hot * np.log(Y_proba_predict))\n",
    "l2_loss = (alpha / 2) * np.sum(np.square(Theta[1:]))\n",
    "cost += l2_loss\n",
    "\n",
    "gradients = (- 1 / m) * (X_with_bias.T @ (Y_one_hot - Y_proba_predict)) + alpha * np.r_[np.zeros((1, K)), Theta[1:]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdALaZI2eRad"
   },
   "source": [
    "### 4. Involving $m$ training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9S7zs0qf-1r"
   },
   "source": [
    "$\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    " --- (\\mathbf{x}^{(1)})^T ---\\\\ \n",
    " --- (\\mathbf{x}^{(2)})^T ---\\\\ \n",
    " \\vdots \\\\\n",
    " --- (\\mathbf{x}^{(m)})^T ---\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\n",
    "\\Theta =\n",
    "\\left[\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YycUl6UlgEks"
   },
   "source": [
    "#### A) ***Compute softmax score / logits***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBVyCLypgRJr"
   },
   "source": [
    "$\n",
    "S(\\mathbf{X}) = \n",
    "\\mathbf{X} \\cdot \\Theta =\n",
    "\\begin{bmatrix}\n",
    " {(\\mathbf{x}^{(1)})}^T\\cdot\\theta^{(1)}  & {(\\mathbf{x}^{(1)})}^T\\cdot\\theta^{(2)} & ... & {(\\mathbf{x}^{(1)})}^T\\cdot\\theta^{(K)}\\\\ \n",
    " {(\\mathbf{x}^{(2)})}^T\\cdot\\theta^{(1)}  & {(\\mathbf{x}^{(2)})}^T\\cdot\\theta^{(2)} & ... & {(\\mathbf{x}^{(2)})}^T\\cdot\\theta^{(K)}\\\\ \n",
    " {(\\mathbf{x}^{(3)})}^T\\cdot\\theta^{(1)}  & {(\\mathbf{x}^{(3)})}^T\\cdot\\theta^{(2)} & ... & {(\\mathbf{x}^{(3)})}^T\\cdot\\theta^{(K)}\\\\ \n",
    " \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
    " {(\\mathbf{x}^{(m)})}^T\\cdot\\theta^{(1)}  & {(\\mathbf{x}^{(m)})}^T\\cdot\\theta^{(2)} & ... & {(\\mathbf{x}^{(m)})}^T\\cdot\\theta^{(K)}\\\\ \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    " s_1(\\mathbf{x}^{(1)}) & s_2(\\mathbf{x}^{(1)}) & ... & s_K(\\mathbf{x}^{(1)})\\\\ \n",
    " s_1(\\mathbf{x}^{(2)}) & s_2(\\mathbf{x}^{(2)}) & ... & s_K(\\mathbf{x}^{(2)})\\\\ \n",
    " s_1(\\mathbf{x}^{(3)}) & s_2(\\mathbf{x}^{(3)}) & ... & s_K(\\mathbf{x}^{(3)})\\\\ \n",
    " \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
    " s_1(\\mathbf{x}^{(m)}) & s_2(\\mathbf{x}^{(m)}) & ... & s_K(\\mathbf{x}^{(m)})\\\\ \n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM0nSElKga2z"
   },
   "source": [
    "#### B) ***Apply `Softmax function` to logits***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqzciZiUhR6a"
   },
   "source": [
    "$\n",
    "\\hat{\\mathbf{P}} =\n",
    "\\exp(S(\\mathbf{X})) / sumByColumns(\\exp(S(\\mathbf{X}))) \\\\   = \n",
    "\\begin{bmatrix}\n",
    " \\exp(s_1(\\mathbf{x}^{(1)})) & \n",
    " \\exp(s_2(\\mathbf{x}^{(1)})) & ... & \n",
    " \\exp(s_K(\\mathbf{x}^{(1)})) \\\\ \n",
    " \\exp(s_1(\\mathbf{x}^{(2)})) & \n",
    " \\exp(s_2(\\mathbf{x}^{(2)})) & ... & \n",
    " \\exp(s_K(\\mathbf{x}^{(2)})) \\\\ \n",
    " \\exp(s_1(\\mathbf{x}^{(3)})) & \n",
    " \\exp(s_2(\\mathbf{x}^{(3)})) & ... & \n",
    " \\exp(s_K(\\mathbf{x}^{(3)})) \\\\ \n",
    " \\vdots & \\vdots  & \\ddots & \\vdots\\\\\n",
    " \\exp(s_1(\\mathbf{x}^{(m)})) & \n",
    " \\exp(s_2(\\mathbf{x}^{(m)})) & ... & \n",
    " \\exp(s_K(\\mathbf{x}^{(m)})) \\\\ \n",
    "\\end{bmatrix} \\div\n",
    "\\begin{bmatrix}\n",
    "\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(1)})) \\\\\n",
    "\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(2)})) \\\\\n",
    "\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(3)})) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(m)})) \\\\\n",
    "\\end{bmatrix} \\\\    =\n",
    "\\begin{bmatrix}\n",
    " \\frac{\\exp(s_1(\\mathbf{x}^{(1)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(1)}))} & \n",
    " \\frac{\\exp(s_2(\\mathbf{x}^{(1)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(1)}))} & ... & \n",
    " \\frac{\\exp(s_K(\\mathbf{x}^{(1)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(1)}))} \\\\ \n",
    " \\frac{\\exp(s_1(\\mathbf{x}^{(2)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(2)}))} & \n",
    " \\frac{\\exp(s_2(\\mathbf{x}^{(2)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(2)}))} & ... & \n",
    " \\frac{\\exp(s_K(\\mathbf{x}^{(2)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(2)}))} \\\\ \n",
    " \\frac{\\exp(s_1(\\mathbf{x}^{(3)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(3)}))} & \n",
    " \\frac{\\exp(s_2(\\mathbf{x}^{(3)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(3)}))} & ... & \n",
    " \\frac{\\exp(s_K(\\mathbf{x}^{(3)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(3)}))} \\\\ \n",
    " \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
    " \\frac{\\exp(s_1(\\mathbf{x}^{(m)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(m)}))} & \n",
    " \\frac{\\exp(s_2(\\mathbf{x}^{(m)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(m)}))} & ... & \n",
    " \\frac{\\exp(s_K(\\mathbf{x}^{(m)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(m)}))} \\\\ \n",
    "\\end{bmatrix} \\\\    =\n",
    "\\begin{bmatrix}\n",
    " \\hat{{p}}_1^{(1)} & \\hat{{p}}_2^{(1)} & ... & \\hat{{p}}_K^{(1)} \\\\ \n",
    " \\hat{{p}}_1^{(2)} & \\hat{{p}}_2^{(2)} & ... & \\hat{{p}}_K^{(2)} \\\\ \n",
    " \\vdots  & \\vdots  & \\ddots & \\vdots\\\\ \n",
    " \\hat{{p}}_1^{(m)} & \\hat{{p}}_2^{(m)} & ... & \\hat{{p}}_K^{(m)} \\\\ \n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D07iX15GhvKT"
   },
   "source": [
    "#### C) ***Make prediction by choosing the class with highest probability***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euSV9Rf5ihBy"
   },
   "source": [
    "$\n",
    "\\hat{y} = argmax_k (\\hat{\\mathbf{P}}) \\\\ = \n",
    "argmax_k (\\begin{bmatrix}\n",
    " --- & \\hat{{p}}^{(1)} & --- \\\\ \n",
    " --- & \\hat{{p}}^{(2)} & --- \\\\ \n",
    " & \\vdots & \\\\ \n",
    " --- & \\hat{{p}}^{(m)} & --- \\\\ \n",
    "\\end{bmatrix}) \\\\ =\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}^{(1)} \\\\\n",
    "\\hat{y}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNNBWtpMlO_P"
   },
   "source": [
    "***References:*** \n",
    "- [Chapter 4 Hands-on Machine Learning with Scikit-Learn, Keras & Tensorflow](https://github.com/ageron/handson-ml2)\n",
    "- http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "55452a8ab240e91628b26dddff71ee1cf7df2f87996c364f4bbe709472e503b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
