{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.3 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.3"
    },
    "interpreter": {
      "hash": "55452a8ab240e91628b26dddff71ee1cf7df2f87996c364f4bbe709472e503b7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8n8o6UyM6KR"
      },
      "source": [
        "# Softmax regression"
      ]
    },
    {
      "source": [
        "<a href=\"https://nbviewer.jupyter.org/github/hongjiaherng/ML-Collections/blob/main/just4funml/notes/note_softmax_reg.ipynb\" \n",
        "   target=\"_parent\">\n",
        "   <img align=\"left\" \n",
        "      src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" \n",
        "      width=\"109\" height=\"20\">\n",
        "</a>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnTTrp7VNNZn"
      },
      "source": [
        "### 1. Brief explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8E7KziAM71E"
      },
      "source": [
        "`Softmax regression`, or `Multinomial logistic regression` is the generalized version of logistic regression which can perform multiclass classification directly, unlike regular logistic regression which needs to use one-versus-all technique to enable multiclass classification.\n",
        "<br><br>\n",
        "\n",
        "Given an instance $x$, this model computes a score $s_k(x)$ for each class $k$ on the instance. Then, `softmax function` is applied to the scores to obtain the probability of instance $x$ belongs to every class. Finally, the model chooses the class which has the highest probability and classify instance $x$ to that particular class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMusLRxyNDX0"
      },
      "source": [
        "### 2. Model hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x90tiCRON1Ab"
      },
      "source": [
        "The following formulas in this part are all based on 1 training example\n",
        "\n",
        "Define:<br>\n",
        "$\\mathbf{x} = \n",
        "\\begin{bmatrix}\n",
        "x_0 \\\\\n",
        "x_1 \\\\\n",
        "\\vdots \\\\\n",
        "x_n \\\\\n",
        "\\end{bmatrix}$ &nbsp;&nbsp;&nbsp;\n",
        "$\\theta^{(k)} =\n",
        "\\begin{bmatrix}\n",
        "\\theta^{(k)}_0 \\\\\n",
        "\\theta^{(k)}_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\theta^{(k)}_n \\\\\n",
        "\\end{bmatrix}\n",
        "$<br>\n",
        "$\n",
        "\\Theta =\n",
        "\\begin{bmatrix}\n",
        "\\theta^{(1)}_0 & \\theta^{(2)}_0 & ... & \\theta^{(K)}_0 \\\\\n",
        "\\theta^{(1)}_1 & \\theta^{(2)}_1 & ... & \\theta^{(K)}_1 \\\\\n",
        "\\theta^{(1)}_2 & \\theta^{(2)}_2 & ... & \\theta^{(K)}_2 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\theta^{(1)}_n & \\theta^{(2)}_n & ... & \\theta^{(K)}_n \\\\\n",
        "\\end{bmatrix} =\n",
        "\\left[\\begin{array}{cccc}| & | & | & | \\\\\n",
        "\\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\\\\n",
        "| & | & | & |\n",
        "\\end{array}\\right]\n",
        "$\n",
        "<br><br>\n",
        "where<br>\n",
        "$n$ = number of features, <br>\n",
        "$K$ = number of classes, <br>\n",
        "$\\mathbf{x} \\in \\mathbb{R}^{(n+1)\\times1}$ &nbsp;, (n+1-dimensional vector include bias term)<br>\n",
        "$\\Theta \\in \\mathbb{R}^{(n+1)\\times K}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjhnlWSCOBvS"
      },
      "source": [
        "a. ***Softmax score for class k of instance $x$ (aka logit)*** \n",
        "    \n",
        "- Compute this for all class k where k = 1, 2, ..., k\n",
        "<br><br>\n",
        "$s_k(\\mathbf{x}) = \\mathbf{x}^T\\theta^{(k)}$\n",
        "\n",
        "<br>where <br>\n",
        "$s_k(\\mathbf{x}) \\in \\mathbb{R}$, <br>\n",
        "$\\mathbf{x}$ = feature vector of an instance, <br>\n",
        "$\\theta^{(k)}$ = weight of class $k$ for this particular instance (Also vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkoyBnrqONMi"
      },
      "source": [
        "b. ***Softmax function*** \n",
        "- Compute this for all class k where k = 1, 2, ..., k\n",
        "<br><br>\n",
        "$\\hat{p}_k = \\sigma(s(\\mathbf{x}))_k = \\dfrac{\\exp(s_k(\\mathbf{x}))}{\\sum\\limits_{j=1}^{K}{\\exp(s_j(\\mathbf{x}))}}$\n",
        "\n",
        "where <br>\n",
        "$K$ = number of classes, <br>\n",
        "$\\sum\\limits_{j=1}^{K}{\\exp(s_j(\\mathbf{x}))}$ = the sum of exponential of softmax scores for every class of instance $\\mathbf{x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGk-sX11X0Xb"
      },
      "source": [
        "c. ***Softmax regression classifier prediction***\n",
        "- This function obtains the maximum entry in the vector and return its class's number $k$\n",
        "<br><br>\n",
        "$\\hat{y} = argmax_{k}<{\\hat{p}_1}, {\\hat{p}_2}, ..., {\\hat{p}_K}>$\n",
        "\n",
        "where <br>\n",
        "$\\hat{y}$ = prediction of instance $\\mathbf{x}$ (class's number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulraatYXYAKb"
      },
      "source": [
        "### 3. Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu4jJWheaCIX"
      },
      "source": [
        "$J(\\Theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)}) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmwrssYQdxYu"
      },
      "source": [
        "$J(\\Theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)}) + C \\sum\\limits_{j=1}^{n} \\sum\\limits_{k=1}^{K} |\\theta_j^{(k)}| $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3fyaNgvbN3B"
      },
      "source": [
        "$J(\\Theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)}) + \\frac{C}{2} \\sum\\limits_{j=1}^{n} \\sum\\limits_{k=1}^{K} (\\theta_j^{(k)})^2 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdALaZI2eRad"
      },
      "source": [
        "### 4. Involve $m$ training examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9S7zs0qf-1r"
      },
      "source": [
        "$\\mathbf{X} =\n",
        "\\begin{bmatrix}\n",
        " --- (\\mathbf{x}^{(1)})^T ---\\\\ \n",
        " --- (\\mathbf{x}^{(2)})^T ---\\\\ \n",
        " \\vdots \\\\\n",
        " --- (\\mathbf{x}^{(m)})^T ---\\\\ \n",
        "\\end{bmatrix}$\n",
        "\n",
        "$\n",
        "\\Theta =\n",
        "\\left[\\begin{array}{cccc}| & | & | & | \\\\\n",
        "\\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\\\\n",
        "| & | & | & |\n",
        "\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YycUl6UlgEks"
      },
      "source": [
        "a. ***Compute softmax score / logits***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBVyCLypgRJr"
      },
      "source": [
        "$\n",
        "S(\\mathbf{X}) = \n",
        "\\mathbf{X} \\cdot \\Theta =\n",
        "\\begin{bmatrix}\n",
        " {(\\mathbf{x}^{(1)})}^T\\cdot\\theta^{(1)}  & {(\\mathbf{x}^{(1)})}^T\\cdot\\theta^{(2)} & ... & {(\\mathbf{x}^{(1)})}^T\\cdot\\theta^{(K)}\\\\ \n",
        " {(\\mathbf{x}^{(2)})}^T\\cdot\\theta^{(1)}  & {(\\mathbf{x}^{(2)})}^T\\cdot\\theta^{(2)} & ... & {(\\mathbf{x}^{(2)})}^T\\cdot\\theta^{(K)}\\\\ \n",
        " {(\\mathbf{x}^{(3)})}^T\\cdot\\theta^{(1)}  & {(\\mathbf{x}^{(3)})}^T\\cdot\\theta^{(2)} & ... & {(\\mathbf{x}^{(3)})}^T\\cdot\\theta^{(K)}\\\\ \n",
        " \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
        " {(\\mathbf{x}^{(m)})}^T\\cdot\\theta^{(1)}  & {(\\mathbf{x}^{(m)})}^T\\cdot\\theta^{(2)} & ... & {(\\mathbf{x}^{(m)})}^T\\cdot\\theta^{(K)}\\\\ \n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        " s_1(\\mathbf{x}^{(1)}) & s_2(\\mathbf{x}^{(1)}) & ... & s_K(\\mathbf{x}^{(1)})\\\\ \n",
        " s_1(\\mathbf{x}^{(2)}) & s_2(\\mathbf{x}^{(2)}) & ... & s_K(\\mathbf{x}^{(2)})\\\\ \n",
        " s_1(\\mathbf{x}^{(3)}) & s_2(\\mathbf{x}^{(3)}) & ... & s_K(\\mathbf{x}^{(3)})\\\\ \n",
        " \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
        " s_1(\\mathbf{x}^{(m)}) & s_2(\\mathbf{x}^{(m)}) & ... & s_K(\\mathbf{x}^{(m)})\\\\ \n",
        "\\end{bmatrix}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM0nSElKga2z"
      },
      "source": [
        "b. ***Apply `Softmax function` to logits***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqzciZiUhR6a"
      },
      "source": [
        "$\n",
        "\\hat{\\mathbf{P}} =\n",
        "\\exp(S(\\mathbf{X})) / sumByColumns(\\exp(S(\\mathbf{X}))) \\\\   = \n",
        "\\begin{bmatrix}\n",
        " \\exp(s_1(\\mathbf{x}^{(1)})) & \n",
        " \\exp(s_2(\\mathbf{x}^{(1)})) & ... & \n",
        " \\exp(s_K(\\mathbf{x}^{(1)})) \\\\ \n",
        " \\exp(s_1(\\mathbf{x}^{(2)})) & \n",
        " \\exp(s_2(\\mathbf{x}^{(2)})) & ... & \n",
        " \\exp(s_K(\\mathbf{x}^{(2)})) \\\\ \n",
        " \\exp(s_1(\\mathbf{x}^{(3)})) & \n",
        " \\exp(s_2(\\mathbf{x}^{(3)})) & ... & \n",
        " \\exp(s_K(\\mathbf{x}^{(3)})) \\\\ \n",
        " \\vdots & \\vdots  & \\ddots & \\vdots\\\\\n",
        " \\exp(s_1(\\mathbf{x}^{(m)})) & \n",
        " \\exp(s_2(\\mathbf{x}^{(m)})) & ... & \n",
        " \\exp(s_K(\\mathbf{x}^{(m)})) \\\\ \n",
        "\\end{bmatrix} \\div\n",
        "\\begin{bmatrix}\n",
        "\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(1)})) \\\\\n",
        "\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(2)})) \\\\\n",
        "\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(3)})) \\\\\n",
        "\\vdots \\\\\n",
        "\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(m)})) \\\\\n",
        "\\end{bmatrix} \\\\    =\n",
        "\\begin{bmatrix}\n",
        " \\frac{\\exp(s_1(\\mathbf{x}^{(1)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(1)}))} & \n",
        " \\frac{\\exp(s_2(\\mathbf{x}^{(1)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(1)}))} & ... & \n",
        " \\frac{\\exp(s_K(\\mathbf{x}^{(1)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(1)}))} \\\\ \n",
        " \\frac{\\exp(s_1(\\mathbf{x}^{(2)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(2)}))} & \n",
        " \\frac{\\exp(s_2(\\mathbf{x}^{(2)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(2)}))} & ... & \n",
        " \\frac{\\exp(s_K(\\mathbf{x}^{(2)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(2)}))} \\\\ \n",
        " \\frac{\\exp(s_1(\\mathbf{x}^{(3)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(3)}))} & \n",
        " \\frac{\\exp(s_2(\\mathbf{x}^{(3)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(3)}))} & ... & \n",
        " \\frac{\\exp(s_K(\\mathbf{x}^{(3)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(3)}))} \\\\ \n",
        " \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n",
        " \\frac{\\exp(s_1(\\mathbf{x}^{(m)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(m)}))} & \n",
        " \\frac{\\exp(s_2(\\mathbf{x}^{(m)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(m)}))} & ... & \n",
        " \\frac{\\exp(s_K(\\mathbf{x}^{(m)}))}{\\sum\\limits_{j=1}^{K}  \\exp(s_j(\\mathbf{x}^{(m)}))} \\\\ \n",
        "\\end{bmatrix} \\\\    =\n",
        "\\begin{bmatrix}\n",
        " \\hat{{p}}_1^{(1)} & \\hat{{p}}_2^{(1)} & ... & \\hat{{p}}_K^{(1)} \\\\ \n",
        " \\hat{{p}}_1^{(2)} & \\hat{{p}}_2^{(2)} & ... & \\hat{{p}}_K^{(2)} \\\\ \n",
        " \\vdots  & \\vdots  & \\ddots & \\vdots\\\\ \n",
        " \\hat{{p}}_1^{(m)} & \\hat{{p}}_2^{(m)} & ... & \\hat{{p}}_K^{(m)} \\\\ \n",
        "\\end{bmatrix}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D07iX15GhvKT"
      },
      "source": [
        "c. ***Make prediction by choosing the class with highest probability***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euSV9Rf5ihBy"
      },
      "source": [
        "$\n",
        "\\hat{y} = argmax_k (\\hat{\\mathbf{P}}) \\\\ = \n",
        "argmax_k (\\begin{bmatrix}\n",
        " --- & \\hat{{p}}^{(1)} & --- \\\\ \n",
        " --- & \\hat{{p}}^{(2)} & --- \\\\ \n",
        " & \\vdots & \\\\ \n",
        " --- & \\hat{{p}}^{(m)} & --- \\\\ \n",
        "\\end{bmatrix}) \\\\ =\n",
        "\\begin{bmatrix}\n",
        "\\hat{y}^{(1)} \\\\\n",
        "\\hat{y}^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{y}^{(m)} \\\\\n",
        "\\end{bmatrix}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNNBWtpMlO_P"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}