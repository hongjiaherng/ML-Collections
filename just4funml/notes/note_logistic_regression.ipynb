{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc50a10",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7e3f9",
   "metadata": {},
   "source": [
    "<a href=\"https://nbviewer.jupyter.org/github/hongjiaherng/ML-Collections/blob/main/just4funml/notes/note_logistic_regression.ipynb\" \n",
    "   target=\"_parent\">\n",
    "   <img align=\"left\" \n",
    "      src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" \n",
    "      width=\"109\" height=\"20\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6f456",
   "metadata": {},
   "source": [
    "## 1. Brief explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8733e",
   "metadata": {},
   "source": [
    "`Logistic regression` is a binary classifier that classify an instance based on its probability that it belongs to a class (says 0 and 1). It will classify an instance to positive class if the probability is >= 0.5 otherwise it will classifier that instance to negative class.\n",
    "<br><br>\n",
    "\n",
    "In fact, we can customize the threshold for classifying instance to meet our special needs. For example, if the we set the threshold to 0.7, then only instances with probability 0.7 or above will be classified into positive class and the rest will be classified as negative class.\n",
    "<br><br>\n",
    "\n",
    "Logistic regression always try to draw a decision boundary to separate out two different class of data. If we ever want to use logistic regression for multiclass classification, then we will need to use `One-vs-all` (aka `One-vs-rest`) technique to achieve it. In simple word, it will train K different model on the data where each model will be able to recognize one particular class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd660d",
   "metadata": {},
   "source": [
    "## 2. Model hypothesis\n",
    "- In this part, we will take one single instance $\\mathbf{x}$ to describe the model hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4bb48f",
   "metadata": {},
   "source": [
    "First, understand that:<br><br>\n",
    "$\n",
    "\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n \\\\\n",
    "\\end{bmatrix}\n",
    ",\n",
    "y = 0\\; or \\;1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2a823",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "- We will plug the weighted sum of x, that is $\\mathbf{z}$ into the `sigmoid function`, and it will map $\\mathbf{z}$ to the range of 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda6a51",
   "metadata": {},
   "source": [
    "$\n",
    "\\mathbf{z} = \\theta^\\top \\mathbf{x}\n",
    "$\n",
    "<br>\n",
    "$\n",
    "\\sigma(\\mathbf{z}) = \\frac{1}{1 + \\mathbf{e}^{-\\mathbf{z}}}\n",
    "$\n",
    "<br><br>\n",
    "\n",
    "$\n",
    "h_\\theta(\\mathbf{x}) = \\sigma(\\mathbf{\\theta^\\top \\mathbf{x}}) = \\frac{1}{1 + \\mathbf{e}^{-\\theta^\\top \\mathbf{x}}}\n",
    "$\n",
    ", where\n",
    "$\n",
    "0 \\leq h_\\theta(\\mathbf{x}) \\leq 1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a236a",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "- The model will predict 1 if the hypothesis is greater than a threshold value, otherwise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0a8c0",
   "metadata": {},
   "source": [
    "$\n",
    "\\hat{y} = \n",
    "\\left\\{\\begin{matrix}\n",
    "1 & if \\; \\; h_\\theta(\\mathbf{x}) >= 0.5 \\\\ \n",
    "0 & if \\; \\;  h_\\theta(\\mathbf{x}) < 0.5\n",
    "\\end{matrix}\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd3b17",
   "metadata": {},
   "source": [
    "## 3. Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5a827",
   "metadata": {},
   "source": [
    "$\n",
    "J(\\theta) = - \\frac{1}{m}\\sum_\\limits{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\\theta(x^{(i)})) \\right ]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb799efd",
   "metadata": {},
   "source": [
    "- Why this cost function? (Try sub value to $h_\\theta(x^{(i)})$ ranging from 0 to 1)\n",
    "    - If $y^{(i)} == 1$, this $log (h_\\theta(x^{(i)}))$ part of equation is used and the back part of the equation is removed, and\n",
    "        - if we predict $h_\\theta(x^{(i)})$ as high, then the cost is lower, because $y^{(i)}$ is in fact belongs to positive class\n",
    "        - if we predict $h_\\theta(x^{(i)})$ as low, then the cost is higher, because $y^{(i)}$ is not belongs to positive class\n",
    "    - If $y^{(i)} == 0$, this $log (1 - h_\\theta(x^{(i)}))$ part of equation is used and the front part is being removed, and\n",
    "        - if we predict $h_\\theta(x^{(i)})$ as low, then the cost is lower, because $y^{(i)}$ is in fact belongs to negative class\n",
    "        - if we predict $h_\\theta(x^{(i)})$ as high, then the cost is higher, because $y^{(i)}$ is not belongs to negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1f3a3",
   "metadata": {},
   "source": [
    "### Minimizing $J(\\theta)$\n",
    "- The following gradient vector is what we need to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd4d23",
   "metadata": {},
   "source": [
    "$\n",
    "\\nabla_{\\theta} J(\\theta) = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial}{\\partial\\theta_0} J(\\theta) \\\\\n",
    "\\dfrac{\\partial}{\\partial\\theta_1} J(\\theta) \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial}{\\partial\\theta_n} J(\\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d240f25",
   "metadata": {},
   "source": [
    "### Compute derivative of $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b91a6d",
   "metadata": {},
   "source": [
    "- First, let's compute the derivative of sigmoid function w.r.t $\\mathbf{z}$ so that we can simplify the process later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad02a2",
   "metadata": {},
   "source": [
    "$\n",
    "\\sigma(\\mathbf{z}) = \\dfrac{1}{1 + \\mathbf{e}^{-z}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea25780",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\dfrac{d\\sigma(\\mathbf{z})}{d\\mathbf{z}} &=\n",
    "\\dfrac{d}{d\\mathbf{z}} \\left( 1 + e^{-\\mathbf{z}} \\right)^{-1} \\\\ &=\n",
    "- \\dfrac{1}{(1 + e^{-\\mathbf{z}})^{2}} (e^{-\\mathbf{z}}) \\dfrac{d}{d\\mathbf{z}} \\left(-\\mathbf{z}\\right) \\\\ &=\n",
    "- \\dfrac{e^{-\\mathbf{z}}}{(1 + e^{-\\mathbf{z}})^{2}} \\left(-1\\right) \\\\ &=\n",
    "\\dfrac{ e^{-\\mathbf{z}} }{ 1 + e^{-\\mathbf{z}} } \\left(\\dfrac{ 1 }{ 1 + e^{-\\mathbf{z}} }\\right) \\\\ &=\n",
    "\\sigma(\\mathbf{z}) \\dfrac{1 + e^{-\\mathbf{z}} - 1 }{1 + e^{-\\mathbf{z}}}\\\\ &=\n",
    "\\sigma(\\mathbf{z})\\left( 1- \\sigma(\\mathbf{z}) \\right)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3fe16b",
   "metadata": {},
   "source": [
    "- Compute the partial derivative of cost function w.r.t. $\\theta_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f0170",
   "metadata": {},
   "source": [
    "$\n",
    "J(\\theta) = - \\frac{1}{m}\\sum_\\limits{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\\theta(x^{(i)})) \\right ]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac9e72",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\\frac{\\partial}{\\partial \\theta_j} J(\\theta) \n",
    "&= \n",
    "\\frac{\\partial}{\\partial \\theta_j} \\frac{-1}{m}\\sum_{i=1}^m \\left [ y^{(i)} log (h_\\theta(\\mathbf{x}^{(i)})) + (1-y^{(i)}) log (1 - h_\\theta(\\mathbf{x}^{(i)})) \\right ] \n",
    "\\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} \\frac{\\partial}{\\partial \\theta_j} log (h_\\theta(\\mathbf{x}^{(i)}))   + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta_j} log (1 - h_\\theta(\\mathbf{x}^{(i)}))\\right ] \n",
    "\\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [     \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j} h_\\theta(\\mathbf{x}^{(i)})}{h_\\theta(\\mathbf{x}^{(i)})}   + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 - h_\\theta(\\mathbf{x}^{(i)}))}{1 - h_\\theta(\\mathbf{x}^{(i)})}\\right ] \n",
    "\\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [     \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j} \\sigma(\\theta^T \\mathbf{x}^{(i)})}{h_\\theta(\\mathbf{x}^{(i)})}   + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 - \\sigma(\\theta^T \\mathbf{x}^{(i)}))}{1 - h_\\theta(\\mathbf{x}^{(i)})}\\right ] \n",
    "\\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [     \\frac{y^{(i)} \\sigma(\\theta^T \\mathbf{x}^{(i)}) (1 - \\sigma(\\theta^T \\mathbf{x}^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T \\mathbf{x}^{(i)}}{h_\\theta(\\mathbf{x}^{(i)})}   + \\frac{- (1-y^{(i)}) \\sigma(\\theta^T \\mathbf{x}^{(i)}) (1 - \\sigma(\\theta^T \\mathbf{x}^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T \\mathbf{x}^{(i)}}{1 - h_\\theta(\\mathbf{x}^{(i)})}\\right ] \n",
    "\\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [     \\frac{y^{(i)} h_\\theta(\\mathbf{x}^{(i)}) (1 - h_\\theta(\\mathbf{x}^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T \\mathbf{x}^{(i)}}{h_\\theta(\\mathbf{x}^{(i)})}   - \\frac{(1-y^{(i)}) h_\\theta(\\mathbf{x}^{(i)}) (1 - h_\\theta(\\mathbf{x}^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T \\mathbf{x}^{(i)}}{1 - h_\\theta(\\mathbf{x}^{(i)})}\\right ] \n",
    "\\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} (1 - h_\\theta(\\mathbf{x}^{(i)})) \\mathbf{x}^{(i)}_j - (1-y^{(i)}) h_\\theta(\\mathbf{x}^{(i)}) \\mathbf{x}^{(i)}_j\\right ] \\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} (1 - h_\\theta(\\mathbf{x}^{(i)})) - (1-y^{(i)}) h_\\theta(\\mathbf{x}^{(i)}) \\right ] \\mathbf{x}^{(i)}_j \n",
    "\\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} - y^{(i)} h_\\theta(\\mathbf{x}^{(i)}) - h_\\theta(\\mathbf{x}^{(i)}) + y^{(i)} h_\\theta(\\mathbf{x}^{(i)}) \\right ] \\mathbf{x}^{(i)}_j \n",
    "\\newline&= \n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - h_\\theta(\\mathbf{x}^{(i)}) \\right ] \\mathbf{x}^{(i)}_j  \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a4a08",
   "metadata": {},
   "source": [
    "### Summary on cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2854a9",
   "metadata": {},
   "source": [
    "#### Cost function (Unregularized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb0ca4",
   "metadata": {},
   "source": [
    "$\n",
    "J(\\theta) = - \\frac{1}{m}\\sum_\\limits{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\\theta(x^{(i)})) \\right ]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f875a6",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &=\n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - h_\\theta(x^{(i)}) \\right ] x^{(i)}_j\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50008dca",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vectorized implementation\n",
    "cost = (-1 / m) * np.sum(y * np.log(y_proba_predict) + (1 - y) * np.log(1 - y_proba_predict))\n",
    "gradients = (-1 / m) * (X_with_bias.T @ (y - y_proba_predict))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4eec7",
   "metadata": {},
   "source": [
    "#### Cost function with $l_2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c90836",
   "metadata": {},
   "source": [
    "$\n",
    "J(\\theta) = - \\frac{1}{m}\\sum_\\limits{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\\theta(x^{(i)})) \\right ] + \\dfrac{ \\lambda }{ 2m } \\sum_\\limits{j=1}^{n} \\theta_j^{2} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c18597",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &=\n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - h_\\theta(x^{(i)}) \\right ] x^{(i)}_j + \\dfrac{\\lambda}{m}\\theta_j \n",
    " \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ j \\in \\lbrace 1,2...n\\rbrace\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83128462",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vectorized implementation\n",
    "cost = (-1 / m) * np.sum(y * np.log(y_proba_predict) + (1 - y) * np.log(1 - y_proba_predict))\n",
    "l2_loss = (alpha / 2 * m) * np.sum(np.square(theta[1:]))\n",
    "cost += l2_loss\n",
    "\n",
    "gradients = (-1 / m) * (X_with_bias.T @ (y - y_proba_predict)) + (alpha / m) * np.r_[0, theta[1:]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e69d6d",
   "metadata": {},
   "source": [
    "#### Cost function with $l_1$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2c18b",
   "metadata": {},
   "source": [
    "$\n",
    "J(\\theta) = - \\frac{1}{m}\\sum_\\limits{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\\theta(x^{(i)})) \\right ] + \\dfrac{ \\lambda }{ m } \\sum_\\limits{j=1}^{n} |\\theta_j|\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798238f",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &=\n",
    "- \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - h_\\theta(x^{(i)}) \\right ] x^{(i)}_j + \\dfrac{\\lambda}{m}sign(\\theta_j) \n",
    " \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ j \\in \\lbrace 1,2...n\\rbrace\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e94d98",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vectorized implementation\n",
    "cost = (-1 / m) * np.sum(y * np.log(y_proba_predict) + (1 - y) * np.log(1 - y_proba_predict))\n",
    "l1_loss = (alpha / m) * np.sum(np.abs(theta[1:]))\n",
    "cost += l1_loss\n",
    "\n",
    "gradients = (-1 / m) * (X_with_bias.T @ (y - y_proba_predict)) + (alpha / m) * np.r_[0, np.sign(theta[1:])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000a4a2",
   "metadata": {},
   "source": [
    "***References:*** \n",
    "- [Coursera Machine Learning Week 3 Resource](https://www.coursera.org/learn/machine-learning/resources/Zi29t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c1396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
